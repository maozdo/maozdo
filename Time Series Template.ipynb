{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time sereis template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"data.csv\", parse_dates=[\"date\"],index_col=\"date\") \n",
    "# parse_dates converts date-like strings to DateTime objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If needed convert string to dateTime object using:\n",
    "pd.to_datetime(data[\"date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "# Set errors to \"coerce\" to mark invalid dates as NaT (not a date, i.e. - missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2020-12-26 00:00:00')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Timestamp(\"2020/12/26\")\n",
    "# The basic date data structure in Pandas is a timestamp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2010-10-31', '2010-11-30', '2010-12-31', '2011-01-31',\n",
       "               '2011-02-28', '2011-03-31', '2011-04-30', '2011-05-31',\n",
       "               '2011-06-30', '2011-07-31',\n",
       "               ...\n",
       "               '2019-12-31', '2020-01-31', '2020-02-29', '2020-03-31',\n",
       "               '2020-04-30', '2020-05-31', '2020-06-30', '2020-07-31',\n",
       "               '2020-08-31', '2020-09-30'],\n",
       "              dtype='datetime64[ns]', length=120, freq='M')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.date_range returns a special DateTimeIndex object that is a collection of TimeStamps with a custom\n",
    "# frequency over a given range\n",
    "pd.date_range(start=\"2010-10-10\", end=\"2020-10-10\", freq=\"M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing time series data can be very intuitive if the index is a DateTimeIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"2010\":\"2015\"] # # All rows within 2010 and 2015\n",
    "# pandas slices dates in closed intervals. For example, using “2010”: “2013” returns rows for all 4 years\n",
    "# it does not exclude the end of the period like integer slicing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean/Median imputation:\n",
    "sk.impute.SimpleImputer(strategy=’mean’).fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffill() / bfill() \n",
    "# forward and backward filling works well on climate and stocks data since the differences between nearby data\n",
    "# points are small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical imputation techniques:\n",
    "pd.interpolate(method=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN:\n",
    "imp = KNNImputer(n_neighbors=k)\n",
    "imp.fit_transform(data[\"cloname\"].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifts and lags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[‘colname’].shift(periods=1)  # the default , Shift forward \n",
    "(lagging) df[‘colname’].shift(periods=-1) # Shift backward \n",
    "df[].diff(periods=1) # Find difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the frequency \n",
    "asfreq(“D”, method=”ffill”) # fills the NANs generated in the new days\n",
    "# Down sampling - changing the frequency from hourly to daily, from daily to weekly, Works like groupby\n",
    "Resample(“M”).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling windows\n",
    "df[\"weekly sum\"].rolling(window=7).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expanding windows\n",
    "df[\"cumulative_sum\"].expanding(min_periods=1).sum()\n",
    "# or\n",
    "df[\"cumulative_sum\"].cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from matplotlib import rcParams\n",
    "\n",
    "decomposition = sm.tsa.seasonal_decompose(df[\"colname\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using sm.tsa.seasonal_decompose on time-series returns a DecomposeResult object with attributes like seasonal, trend and resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for trend property of time series:\n",
    "trend_dict = {}\n",
    "for ts in df.columns:\n",
    "    decomposition = sm.tsa.seasonal_decompose(df[ts].dropna())\n",
    "    # Store back the results\n",
    "    trend_dict[ts] = decomposition.trend\n",
    "\n",
    "pd.DataFrame(trend_dict).plot(subplots=True, layout=(4, 3), linewidth=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For feature with different scales, we should normalize:\n",
    "\n",
    "When normalizing time series, you divide every data point in the distribution by the first sample.\n",
    "\n",
    "This has the effect of representing every single data point as the percentage increase relative to the first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.div(df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of two different time series to detect if there is a correlation between metrics with the same maximum and minimum values (called cross-correaltion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom palette\n",
    "cmap = sns.diverging_palette(250, 15, s=75, l=40, n=9, center=\"light\", as_cmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute corr matrix\n",
    "matrix = df.corr(method=\"pearson\")\n",
    "# Create a mask\n",
    "mask = np.triu(np.ones_like(matrix, dtype=bool))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "sns.heatmap(matrix, mask=mask, cmap=cmap, square=True, annot=True, fmt=\".2f\", ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or even batter compare ts components:\n",
    "\n",
    "seasonality_dict = {\n",
    "    ts: sm.tsa.seasonal_decompose(df[ts].dropna()).seasonal for ts in df.columns\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute corr matrix\n",
    "seasonality_corr = pd.DataFrame(seasonality_dict).corr()\n",
    "sns.clustermap(seasonality_corr, annot=True, square=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ClusterMap rather than a heatmap to see closely correlated groups with the help of dendrograms immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADF stationarity test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns: {Test statistic, P-value, Num lags used, {Critical values}, Estmation of maximized information criteria}\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# If the P-Value is above the threshold of 0.05, we can not reject H₀, so the data is not stationary\n",
    "def adf_test(df):\n",
    "    result = adfuller(df.values)\n",
    "    print('ADF Statistics: %f' % result[0])\n",
    "    print('p-value: %f' % result[1])\n",
    "    print('Critical values:')\n",
    "    for key, value in result[4].items():\n",
    "        print('\\t%s: %.3f' % (key, value))\n",
    "\n",
    "print('ADF Test: $df$ time series')\n",
    "adf_test(df['colname'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KPSS stationarity test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import kpss\n",
    "# The interpretaion of p-value is just the opposite to the ADF test\n",
    "# If the P-Value is below the threshold of 0.05, we can not reject H₀, so the data is not stationary\n",
    "\n",
    "def kpss_test(df):    \n",
    "    statistic, p_value, n_lags, critical_values = kpss(df.values)\n",
    "    \n",
    "    print(f'KPSS Statistic: {statistic}')\n",
    "    print(f'p-value: {p_value}')\n",
    "    print(f'num lags: {n_lags}')\n",
    "    print('Critial Values:')\n",
    "    for key, value in critical_values.items():\n",
    "        print(f'   {key} : {value}')\n",
    "\n",
    "print('KPSS Test: $df$ time series')\n",
    "kpss_test(df['colname'])\n",
    "\n",
    "# to turn ON the stationarity testing around a trend, you need to explicitly pass the regression='ct'\n",
    "kpss(df[‘colname’],regression='ct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First and second order difference\n",
    "df['colname_Diff1'] = df['colname'].diff()\n",
    "df['colname_Diff2'] = df['colname'].diff(2)\n",
    "\n",
    "# Don't forget to drop missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# another option:\n",
    "df_transformed = df.diff().dropna()\n",
    "\n",
    "fig = px.line(df_transformed, facet_col=\"colname\", facet_col_wrap=1)\n",
    "fig.update_yaxes(matches=None)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invert differencing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT TESTED !!!\n",
    "# https://towardsdatascience.com/a-quick-introduction-on-granger-causality-testing-for-time-series-analysis-7113dc9420d2\n",
    "# taken from this url, uses train and test sets based on n_obs\n",
    "#n_obs = 20\n",
    "#df_train, df_test = df[0:-n_obs], df[-n_obs:]\n",
    "\n",
    "lag_order = results.k_ar # choose the appropriate lag from VAR statistic below\n",
    "\n",
    "df_input = df_transformed.values[-lag_order:]\n",
    "df_forecast = results.forecast(y=df, steps=n_obs)\n",
    "df_forecast = (pd.DataFrame(df_forecast, index=df_test.index, columns=df_test.columns + '_pred'))\n",
    "\n",
    "def invert_transformation(df, pred):\n",
    "    forecast = df_forecast.copy()\n",
    "    columns = df.columns\n",
    "    for col in columns:\n",
    "        forecast[str(col)+'_pred'] = df[col].iloc[-1] + forecast[str(col)+'_pred'].cumsum()\n",
    "    return forecast\n",
    "output = invert_transformation(df_train, df_forecast)\n",
    "\n",
    "combined = pd.concat([output['apple_pred'], df_test['apple'], output['walmart_pred'], df_test['walmart'], output['tesla_pred'], df_test['tesla']], axis=1)\n",
    "view raw invert_transform.py hosted with ❤ by GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics metrics\n",
    "#### VAR Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "model = VAR(df_train_transformed)\n",
    "for i in [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]:\n",
    "    result = model.fit(i)\n",
    "    print('Lag Order =', i)\n",
    "    print('AIC : ', result.aic)\n",
    "    print('BIC : ', result.bic)\n",
    "    print('FPE : ', result.fpe)\n",
    "    print('HQIC: ', result.hqic, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no hard-and-fast-rule on the choice of lag order. It is basically an empirical issue. However, it is often advised to use the AIC in selecting the lag order with the smallest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.fit(maxlags=15, ic='aic')\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Durbin-Watson Statistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "out = durbin_watson(results.resid)\n",
    "\n",
    "for col, val in zip(df.columns, out):\n",
    "    print(col, ':', round(val, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Granger Causality Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "maxlag=15\n",
    "test = 'ssr_chi2test'\n",
    "\n",
    "def grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):    \n",
    "   \n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in df.columns:\n",
    "        for r in df.index:\n",
    "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "            min_p_value = np.min(p_values)\n",
    "            df.loc[r, c] = min_p_value\n",
    "    df.columns = [var + '_x' for var in variables]\n",
    "    df.index = [var + '_y' for var in variables]\n",
    "    return df\n",
    "\n",
    "grangers_causation_matrix(df_transformed, variables = df_transformed.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto-Correlation is the comparison of a time series with itself at a different time.\n",
    "When a clear trend exists in a time series, the autocorrelation tends to be high at small lags like 1 or 2. When seasonality exists, the autocorrelation goes up periodically at larger lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics import tsaplots\n",
    "\n",
    "rcParams[\"figure.figsize\"] = 10, 6\n",
    "\n",
    "# Stands for Time Series Analysis Plots (TSA Plots)\n",
    "fig = tsaplots.plot_acf(tps[\"deg_C\"], lags=60)\n",
    "\n",
    "plt.xlabel(\"Lag at k\")\n",
    "plt.ylabel(\"Correlation coefficient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Y axis is the amount of correlation at each lag k. The shaded red region is a confidence interval — if the height of the bars is outside this region, it means the correlation is statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Autocorrelation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial autocorrelation at lag k is the correlation that results after removing the effect of any correlations due to the terms at shorter lags.\n",
    "\n",
    "The autocorrelation for an observation and an observation at a prior time step is comprised of both the direct correlation and indirect correlations. These indirect correlations are a linear function of the correlation of the observation, with observations at intervening time steps.\n",
    "It is these indirect correlations that the partial autocorrelation function seeks to remove.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = tsaplots.plot_pacf(df[\"colname \"], lags=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference is that this method tries to account for the effect the intervening lags have.\n",
    "\n",
    "For example, at lag 3, partial autocorrelation removes the effect lags 1 and 2 have on computing the correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While autocorrelation is useful for analyzing a time series's properties and choosing what type of ARIMA model to use, partial autocorrelation tells what order of autoregressive model to fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search for ARIMA parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the p, d and q parameters to take any value between 0 and 2\n",
    "p = d = q = range(0, 2)\n",
    "\n",
    "# Generate all different combinations of p, q and q triplets\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "# Generate all different combinations of seasonal p, q and q triplets\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n",
    "\n",
    "print('Examples of parameter combinations for Seasonal ARIMA...')\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below iterates through combinations of parameters and uses the SARIMAX function from statsmodels to fit the corresponding Seasonal ARIMA model. Here, the order argument specifies the (p, d, q) parameters, while the seasonal_order argument specifies the (P, D, Q, S) seasonal component of the Seasonal ARIMA model. After fitting each SARIMAX()model, the code prints out its respective AIC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n",
    "\n",
    "for param in pdq:\n",
    "    for param_seasonal in seasonal_pdq:\n",
    "        try:\n",
    "            mod = sm.tsa.statespace.SARIMAX(y,\n",
    "                                            order=param,\n",
    "                                            seasonal_order=param_seasonal,\n",
    "                                            enforce_stationarity=False,\n",
    "                                            enforce_invertibility=False)\n",
    "\n",
    "            results = mod.fit()\n",
    "\n",
    "            print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = sm.tsa.statespace.SARIMAX(y,\n",
    "                                order=(1, 1, 1),  # plug in the parameters from grid search\n",
    "                                seasonal_order=(1, 1, 1, 12), # plug in the parameters from grid search\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "\n",
    "results = mod.fit()\n",
    "\n",
    "print(results.summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to find minimum AIC\n",
    "warnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n",
    "\n",
    "AIC_list = pd.DataFrame({}, columns=['param','param_seasonal','AIC'])\n",
    "for param in pdq:\n",
    "    for param_seasonal in seasonal_pdq:\n",
    "        try:\n",
    "            mod = sm.tsa.statespace.SARIMAX(y,\n",
    "                                            order=param,\n",
    "                                            seasonal_order=param_seasonal,\n",
    "                                            enforce_stationarity=False,\n",
    "                                            enforce_invertibility=False)\n",
    "\n",
    "            results = mod.fit()\n",
    "\n",
    "            print('ARIMA{}x{} - AIC:{}'.format(param, param_seasonal, results.aic))\n",
    "            \n",
    "            temp = pd.DataFrame([[ param ,  param_seasonal , results.aic ]], columns=['param','param_seasonal','AIC'])\n",
    "            AIC_list = AIC_list.append( temp, ignore_index=True)  # DataFrame append\n",
    "            del temp\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n",
    "m = np.amin(AIC_list['AIC'].values) # Find minimum value in AIC\n",
    "l = AIC_list['AIC'].tolist().index(m) # Find index number for lowest AIC\n",
    "Min_AIC_list = AIC_list.iloc[l,:]\n",
    "\n",
    "\n",
    "\n",
    "mod = sm.tsa.statespace.SARIMAX(y,\n",
    "                                order=Min_AIC_list['param'],\n",
    "                                seasonal_order=Min_AIC_list['param_seasonal'],\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "results = mod.fit()\n",
    "\n",
    "print(\"### Min_AIC_list ### \\n{}\".format(Min_AIC_list))\n",
    "\n",
    "print(results.summary().tables[1])\n",
    "\n",
    "results.plot_diagnostics(figsize=(15, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot_diagnostics\n",
    "# We should always run model diagnostics to investigate any unusual behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test spilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 24 # set size of test = number of observations\n",
    "\n",
    "df_train = df[:-test_size]\n",
    "df_test = df[-test_size:]\n",
    "\n",
    "plt.title('Df train and test sets', size=20)\n",
    "plt.plot(df_train, label='Training set')\n",
    "plt.plot(df_test, label='Test set', color='orange')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating Forecasts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = results.get_prediction(start=pd.to_datetime('1998-01-01'), dynamic=False)\n",
    "pred_ci = pred.conf_int()\n",
    "# The code above requires the forecasts to start at January 1998.\n",
    "# The dynamic=False argument ensures that we produce one-step ahead forecasts, meaning that forecasts at\n",
    "# each point are generated using the full history up to that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the real and forecasted values of the data time series to assess how well we did.\n",
    "# zoom in on the end of the time series by slicing the date index.\n",
    "\n",
    "ax = y['1990':].plot(label='observed')\n",
    "pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7)\n",
    "\n",
    "# Confidence Interval\n",
    "ax.fill_between(pred_ci.index,\n",
    "                pred_ci.iloc[:, 0],\n",
    "                pred_ci.iloc[:, 1], color='k', alpha=.2)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('CO2 Levels')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use MSE to check accuracy of our forecasts\n",
    "\n",
    "or RMSE which tells you how many units your model is wrong on average.\n",
    "\n",
    "MAPE tells you how wrong your forecasts are percentage-wise. For example, the MAPE value of 0.02 means your forecasts are 98% accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "#define RMSE function\n",
    "rmse = lambda act, pred: np.sqrt(mean_squared_error(act, pred))\n",
    "\n",
    "y_forecasted = pred.predicted_mean\n",
    "y_truth = y['1998-01-01':] # enter the start date for accuracy check\n",
    "\n",
    "# Compute the mean square error\n",
    "mse = mean_squared_error(y_true = y_truth, y_pred = y_forecasted)\n",
    "rmse = (y_truth, y_forecasted)\n",
    "mape = mean_absolute_percentage_error(y_true = y_truth, y_pred = y_forecasted\n",
    "                                     )\n",
    "print('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))\n",
    "print('The Root Mean Squared Error of our forecasts is {}'.format(round(rmse, 2)))\n",
    "print('The Absolute Percentage Error of our forecasts is {}'.format(round(mape, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dynamic forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dynamic = results.get_prediction(start=pd.to_datetime('1998-01-01'), dynamic=True, full_results=True)\n",
    "pred_dynamic_ci = pred_dynamic.conf_int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get forecast 500 steps ahead in future\n",
    "pred_uc = results.get_forecast(steps=500)\n",
    "\n",
    "# Get confidence intervals of forecasts\n",
    "pred_ci = pred_uc.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = y.plot(label='observed', figsize=(20, 15))\n",
    "pred_uc.predicted_mean.plot(ax=ax, label='Forecast')\n",
    "ax.fill_between(pred_ci.index,\n",
    "                pred_ci.iloc[:, 0],\n",
    "                pred_ci.iloc[:, 1], color='k', alpha=.25)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Variable name')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Modeling with Prophet\n",
    "\n",
    "forecasting tool Prophet is designed for analyzing time-series that display patterns on different time scales such as yearly, weekly and daily. It also has advanced capabilities for modeling the effects of holidays on a time-series and implementing custom changepoints. Therefore, we are using Prophet to get a model up and running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "\n",
    "Prophet_model = Prophet(interval_width=0.95)\n",
    "Prophet_model.fit(df)\n",
    "\n",
    "ts_forecast = Prophet_model.make_future_dataframe(periods=36, freq='MS')\n",
    "ts_forecast = Prophet_model.predict(ts_forecast)\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "Prophet_model.plot(ts_forecast, xlabel = 'Date', ylabel = 'Variable Name')\n",
    "# plt.title('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_forecast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prophet_model.plot_components(ts_forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kats Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kats is a lightweight, easy-to-use, and generalizable framework to perform time series analysis in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kats currently supports the following 10 forecasting models:\n",
    "\n",
    "Linear\n",
    "\n",
    "Quadratic\n",
    "\n",
    "ARIMA\n",
    "\n",
    "SARIMA\n",
    "\n",
    "Holt-Winters\n",
    "\n",
    "Prophet\n",
    "\n",
    "AR-Net\n",
    "\n",
    "LSTM\n",
    "\n",
    "Theta\n",
    "\n",
    "VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfts = df[\"colname\"].to_frame()\n",
    "\n",
    "# Split data into train and test set\n",
    "train_len = 24 # Set test size\n",
    "train = dfts.iloc[:train_len]\n",
    "test = dfts.iloc[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kats.consts import TimeSeriesData\n",
    "\n",
    "# Construct TimeSeriesData object\n",
    "ts = TimeSeriesData(train.reset_index(), time_col_name=\"month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Time Series Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kats.tsfeatures.tsfeatures import TsFeatures\n",
    "\n",
    "model = TsFeatures()\n",
    "\n",
    "output_features = model.transform(ts)\n",
    "output_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFTDetector — Fast Fourier Transform Seasonality Detector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kats also allows us to use Fast Fourier Transform to detect seasonality and find out the potential cycle’s length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kats.detectors.seasonality import FFTDetector\n",
    "\n",
    "fft_detector = FFTDetector(ts)\n",
    "fft_detector.detector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forcast Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet Model\n",
    "from kats.models.prophet import ProphetModel, ProphetParams\n",
    "\n",
    "# Specify parameters\n",
    "params = ProphetParams(seasonality_mode=\"multiplicative\")\n",
    "\n",
    "# Create a model instance\n",
    "m = ProphetModel(ts, params)\n",
    "\n",
    "# Fit mode\n",
    "m.fit()\n",
    "\n",
    "# Forecast\n",
    "fcst = m.predict(steps=30, freq=\"MS\")\n",
    "fcst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOLT-WINTERS Model\n",
    "from kats.models.holtwinters import HoltWintersParams, HoltWintersModel\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "params = HoltWintersParams(\n",
    "            trend=\"add\",\n",
    "            seasonal=\"mul\",\n",
    "            seasonal_periods=12,\n",
    "        )\n",
    "m = HoltWintersModel(\n",
    "    data=ts, \n",
    "    params=params)\n",
    "\n",
    "m.fit()\n",
    "fcst = m.predict(steps=30, alpha = 0.1)\n",
    "m.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Change Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kats.consts import TimeSeriesData, TimeSeriesIterator\n",
    "from kats.detectors.cusum_detection import CUSUMDetector\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "detector = CUSUMDetector(ts)\n",
    "\n",
    "change_points = detector.detector(change_directions=[\"increase\", \"decrease\"])\n",
    "print(\"The change point is on\", change_points[0][0].start_time)\n",
    "\n",
    "# plot the results\n",
    "plt.xticks(rotation=45)\n",
    "detector.plot(change_points)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOCPDetector — Detect Sudden Changes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kats.detectors.bocpd import BOCPDetector, BOCPDModelType, TrendChangeParameters\n",
    "\n",
    "bocpd_detector = BOCPDetector(ts)\n",
    "\n",
    "changepoints = bocpd_detector.detector(\n",
    "    model=BOCPDModelType.NORMAL_KNOWN_MODEL, changepoint_prior=0.01\n",
    "# Distribution options: Normal, Trend Change, Poisson Process Model\n",
    ")\n",
    "for changepoint in changepoints:\n",
    "    print(changepoint[0])\n",
    "    \n",
    "# Plot\n",
    "bocpd_detector.plot(changepoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kats.detectors.outlier import OutlierDetector\n",
    "\n",
    "# Get time series object\n",
    "ts = get_ts(\"nlp\")\n",
    "\n",
    "# Detect outliers\n",
    "ts_outlierDetection = OutlierDetector(ts, \"additive\")\n",
    "ts_outlierDetection.detector()\n",
    "\n",
    "# Print outliers\n",
    "outlier_range1 = ts_outlierDetection.outliers[0]\n",
    "print(f\"The outliers range from {outlier_range1[0]} to {outlier_range1[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_outliers_interpolated = ts_outlierDetection.remover(interpolate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "ax = ts.to_dataframe().plot(x=\"time\", y=\"value\")\n",
    "ts_outliers_interpolated.to_dataframe().plot(x=\"time\", y=\"y_0\", ax=ax)\n",
    "plt.legend(labels=[\"original ts\", \"ts with removed outliers\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
